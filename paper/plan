Лит. обзор + главы введения:
    1. Мы акцентировали внимание на beam search. Нужно сменить вектор, т.к. кажется в эту сторону мы ничего не получили.
    2. В лит обзоре не было ссылки на Малинина (но будем обсуждать) надо добавить
    3. Мб стоит добавить тейки про трансформер
    4. Мб стоит добавить тейки про ансамблирование


Введение
Раздел про перевод (обзор литературы про перевод + прокинуты дорожки к нашей работе)
Раздел про uncertainty 

Основная часть:
    1. Постановка:
        Есть машинный перевод, есть uncertainty. Показывает ошибки. Ссылка на Малинина. но в работе не исследован token-level. 
        Описание моделей, фреймворка, данных, конкретной задачи и конечных целей чуть более формально и кратко
        Не забыть про lenpen label smooth

    2. небольшой пассаж про оценку неопределенностей и разницу между token-level и seq-level (
        Пассаж про то, что мы рассматриваем именно token-level (возможно со ссылками на Малинина)

    3. Графики (beam size, BLEU) для трансформера и конвов. 
        Пассаж про то, что мы хотели бы это исправить оценкой неопределенности
        не забыть дальше указать почему у нас это не получилось
        не забыть сказать, что мы смотрим на трансформер

    4. Non-vocabulary estimations
        4.1 Ensemble softmax variance (correct prefix - first error)
                Что это такое. графики + описание. Почему они такие.
                Рассмотреть кейсы когда мы не можем отделить формы софтмаксов по графикам
                Почему оценивать дисперсию плохо. Плавный переход на энтропию
        4.2 Ensemble softmax entropy (shannon) (correct prefix - first error)
                --//--
                Пассаж про то, что плохо смотреть на графики для correct prefix. Плавный переход на correct token - first error
        4.3 То же, но для correct token. Чуть более кратко.
        4.4 Эксперимент Малинина по детекции ошибочной классификации токена (В начало, в качестве затравки и мотивации)

    5. Vocabulary estimations 
        Затравка про перевзве вероятностей. Выдвигаем гипотезу, что возможно неопределенность токена коррелирует с занижением вероятностей
        5.1 Inensemble variance (correct token - first error)
            --//--
            Затравка про Inensemble probs
        5.2 Inensemble probs (correct token - first error)
            --//--
            Затравка, что хотелось бы смотреть на соответствия (scatters)
        5.3 Inensemble scatters (probs - variance)
            --//--
            Можно добавить еще графики разностей

    6. Перевзвес
        Что хотели бы получить как конечный результат
        Показать предыдущие графики для random tokens, top-tokens
        6.1 Попытки перевзвеса (графики и описание почему не залетело)

Что хочется еще впихнуть, но не понятно куда:
    1. Wrong suffix статистики
    2. Считали все для разных бимов, надо отдельным пунктом показать, что от beam ничего не зависело в конечном итоге
    3. Одна модель ведет себя как Ансамбль, кажется, красивое наблюдение.


# ----------------------------

! Не пишем про ensemble softmax variance.

1. Введение
2. Раздел про МП
3. Раздел про uncertainty
4. Выводы по обзору + связь с нашей работой

Основная часть
5. Общие слова
6. Описание постановки экспериментов (бейзлайны, фреймворки + вставить для конвов и т.п.) Показать что мы обучали сами сетки, сравнивали
Не забыть про lenpen + label smoothing 
    3. Графики (beam size, BLEU) для трансформера и конвов. 
        Пассаж про то, что мы хотели бы это исправить оценкой неопределенности
        не забыть дальше указать почему у нас это не получилось
        не забыть сказать, что мы смотрим на трансформер

7. Эксперимент Малинин (не только TU + KL + DU)

