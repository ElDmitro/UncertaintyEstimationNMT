{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1001\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1001\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };var element = document.getElementById(\"1001\");\n",
       "  if (element == null) {\n",
       "    console.error(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.3.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.3.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.3.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.3.4.min.js\"];\n",
       "  var css_urls = [];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {} // ensure no trailing comma for IE\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };var element = document.getElementById(\"1001\");\n  if (element == null) {\n    console.error(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n    return false;\n  }\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.3.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.3.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.3.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.3.4.min.js\"];\n  var css_urls = [];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import interact, interactive, fixed, interact_manual, widgets\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import importlib\n",
    "\n",
    "from fairseq import bleu, data, options, progress_bar, tasks, tokenizer, utils\n",
    "from fairseq.meters import StopwatchMeter, TimeMeter\n",
    "from fairseq.sequence_generator import SequenceGenerator\n",
    "from fairseq.sequence_scorer import SequenceScorer\n",
    "from bokeh.io import output_notebook, show\n",
    "import networkx as nx\n",
    "from bokeh.models import Plot, Range1d, MultiLine, Circle, HoverTool, TapTool, BoxSelectTool\n",
    "from bokeh.models.graphs import from_networkx, NodesAndLinkedEdges, EdgesAndLinkedNodes\n",
    "from bokeh.palettes import Spectral4\n",
    "import math\n",
    "from itertools import islice,zip_longest\n",
    "from sacrebleu import sentence_bleu, corpus_bleu\n",
    "from tqdm import tqdm_notebook\n",
    "import pandas as pd\n",
    "\n",
    "from bokeh.io import show, output_file\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.models import GraphRenderer, StaticLayoutProvider, Oval, MultiLine\n",
    "from bokeh.models import Plot, Range1d, MultiLine, Circle, HoverTool,\\\n",
    "    TapTool, BoxSelectTool,PanTool,BoxZoomTool\n",
    "from bokeh.models import ColumnDataSource, Range1d, LabelSet, Label, AdaptiveTicker\n",
    "from bokeh.palettes import Spectral8\n",
    "from bokeh.models import SingleIntervalTicker, LinearAxis\n",
    "\n",
    "\n",
    "# from interactive import *\n",
    "from fairseq.models import FairseqIncrementalDecoder\n",
    "\n",
    "from subword_nmt.apply_bpe import BPE\n",
    "from sacremoses import MosesTokenizer\n",
    "\n",
    "import codecs   \n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = 'wmt19.en-ru.ensemble'\n",
    "CHECKPOINT_PATHS = [\n",
    "    'model1.pt',\n",
    "    'model2.pt',\n",
    "    'model3.pt',\n",
    "    'model4.pt',\n",
    "]\n",
    "CHECKPOINT_PATHS = [os.path.join(MODEL_PATH, path) for path in CHECKPOINT_PATHS]\n",
    "\n",
    "MODEL_PATH = ':'.join(CHECKPOINT_PATHS)\n",
    "BINARY_DATA_PATH = 'data-bin/wmt17_en_ru/'\n",
    "# BPECODES_PATH = 'wmt19.en-ru.ensemble/codes'\n",
    "BPECODES_PATH = 'data/wmt17_en_ru/code' # иначе лажа какая-то получается\n",
    "BEAM = '5'\n",
    "LENPEN = '0.6'\n",
    "DIVERSE_BEAM_STRENGTH = '0'\n",
    "SHARED_BPE = True\n",
    "SRS = \"en\"\n",
    "TGT = \"ru\"\n",
    "\n",
    "tkn = {}\n",
    "bpe = {}\n",
    "if not SHARED_BPE:\n",
    "    for l in [SRS, TGT]:\n",
    "        with codecs.open(BPECODES_PATH) as src_codes:\n",
    "            tkn[l] = MosesTokenizer(lang=l)\n",
    "            bpe[l] = BPE(src_codes)\n",
    "else:\n",
    "    l = SRS\n",
    "    with codecs.open(BPECODES_PATH) as src_codes:\n",
    "        tkn[l] = MosesTokenizer(lang=l)\n",
    "        bpe[l] = BPE(src_codes)\n",
    "\n",
    "def prepare_input(s, l='en'):\n",
    "    return [bpe[l].process_line(tkn[l].tokenize(s, return_str=True))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| [en] dictionary: 31640 types\n",
      "| [ru] dictionary: 31232 types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dkuznetsov/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:18: UserWarning: utils.load_ensemble_for_inference is deprecated. Please use checkpoint_utils.load_model_ensemble instead.\n"
     ]
    }
   ],
   "source": [
    "parser = options.get_generation_parser(interactive=True)\n",
    "\n",
    "args = options.parse_args_and_arch(parser, input_args=[\n",
    "    BINARY_DATA_PATH,\n",
    "    '--path', MODEL_PATH,\n",
    "    '--diverse-beam-strength', DIVERSE_BEAM_STRENGTH,\n",
    "    '--lenpen', 0,\n",
    "    '--remove-bpe',\n",
    "    '--beam', BEAM\n",
    "])\n",
    "\n",
    "use_cuda = False\n",
    "task = tasks.setup_task(args)\n",
    "model_paths = args.path.split(':')\n",
    "models, model_args = utils.load_ensemble_for_inference(\n",
    "        model_paths,\n",
    "        task,\n",
    "        model_arg_overrides=eval(args.model_overrides)\n",
    ")\n",
    "tgt_dict = task.target_dictionary\n",
    "\n",
    "\n",
    "for model in models:\n",
    "    model.make_generation_fast_(\n",
    "        beamable_mm_beam_size=None if args.no_beamable_mm else args.beam,\n",
    "        need_attn=args.print_alignment,\n",
    "    )\n",
    "    if args.fp16:\n",
    "        model.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Batch = namedtuple('Batch', 'ids src_tokens src_lengths')\n",
    "Translation = namedtuple('Translation', 'src_str hypos pos_scores alignments')\n",
    "\n",
    "\n",
    "def make_batches(lines, args, task, max_positions):\n",
    "    tokens = [\n",
    "        task.source_dictionary.encode_line(\n",
    "            src_str, add_if_not_exist=False\n",
    "        ).long()\n",
    "        for src_str in lines\n",
    "    ]\n",
    "    lengths = [t.numel() for t in tokens]\n",
    "    itr = task.get_batch_iterator(\n",
    "        dataset=task.build_dataset_for_inference(tokens, lengths),\n",
    "        max_tokens=args.max_tokens,\n",
    "        max_sentences=args.max_sentences,\n",
    "        max_positions=max_positions,\n",
    "        ignore_invalid_inputs=args.skip_invalid_size_inputs_valid_test\n",
    "    ).next_epoch_itr(shuffle=False)\n",
    "    for batch in itr:\n",
    "        yield Batch(\n",
    "            ids=batch['id'],\n",
    "            src_tokens=batch['net_input']['src_tokens'], src_lengths=batch['net_input']['src_lengths'],\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load alignment dictionary for unknown word replacement\n",
    "# (None if no unknown word replacement, empty if no path to align dictionary)\n",
    "align_dict = utils.load_align_dict(args.replace_unk)\n",
    "\n",
    "def make_result(src_str, hypos):\n",
    "    result = Translation(\n",
    "        src_str='O\\t{}'.format(src_str),\n",
    "        hypos=[],\n",
    "        pos_scores=[],\n",
    "        alignments=[],\n",
    "    )\n",
    "\n",
    "    # Process top predictions\n",
    "    for hypo in hypos[:min(len(hypos), args.nbest)]:\n",
    "        hypo_tokens, hypo_str, alignment = utils.post_process_prediction(\n",
    "            hypo_tokens=hypo['tokens'].int().cpu(),\n",
    "            src_str=src_str,\n",
    "            alignment=hypo['alignment'].int().cpu() if hypo['alignment'] is not None else None,\n",
    "            align_dict=align_dict,\n",
    "            tgt_dict=tgt_dict,\n",
    "            remove_bpe=args.remove_bpe,\n",
    "        )\n",
    "        result.hypos.append('H\\t{}\\t{}'.format(hypo['score'], hypo_str))\n",
    "        result.pos_scores.append('P\\t{}'.format(\n",
    "            ' '.join(map(\n",
    "                lambda x: '{:.4f}'.format(x),\n",
    "                hypo['positional_scores'].tolist(),\n",
    "            ))\n",
    "        ))\n",
    "        result.alignments.append(\n",
    "            'A\\t{}'.format(' '.join(map(lambda x: str(utils.item(x)), alignment)))\n",
    "            if args.print_alignment else None\n",
    "        )\n",
    "    return result\n",
    "\n",
    "def process_batch(batch):\n",
    "    tokens = batch.tokens\n",
    "    lengths = batch.lengths\n",
    "\n",
    "    if use_cuda:\n",
    "        tokens = tokens.cuda()\n",
    "        lengths = lengths.cuda()\n",
    "\n",
    "    encoder_input = {'src_tokens': tokens, 'src_lengths': lengths}\n",
    "    translations = translator.generate(\n",
    "        encoder_input,\n",
    "        maxlen=int(args.max_len_a * tokens.size(1) + args.max_len_b),\n",
    "    )\n",
    "\n",
    "    return [make_result(batch.srcs[i], t) for i, t in enumerate(translations)]\n",
    "\n",
    "max_positions = utils.resolve_max_positions(\n",
    "    task.max_positions(),\n",
    "    *[model.max_positions() for model in models]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tmp_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def shannon_entropy(pk, dim=None):\n",
    "    if dim is None:\n",
    "        return -torch.sum(pk * torch.log(pk))\n",
    "\n",
    "    return -torch.sum(pk * torch.log(pk), dim=dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class EnsembleModel(torch.nn.Module):\n",
    "    \"\"\"A wrapper around an ensemble of models.\"\"\"\n",
    "\n",
    "    def __init__(self, models):\n",
    "        super().__init__()\n",
    "        self.models = torch.nn.ModuleList(models)\n",
    "        self.incremental_states = None\n",
    "        if all(hasattr(m, 'decoder') and isinstance(m.decoder, FairseqIncrementalDecoder) for m in models):\n",
    "            self.incremental_states = {m: {} for m in models}\n",
    "\n",
    "    def has_encoder(self):\n",
    "        return hasattr(self.models[0], 'encoder')\n",
    "\n",
    "    def max_decoder_positions(self):\n",
    "        return min(m.max_decoder_positions() for m in self.models)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward_encoder(self, encoder_input):\n",
    "        if not self.has_encoder():\n",
    "            return None\n",
    "        return [model.encoder(**encoder_input) for model in self.models]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward_decoder(self, tokens, encoder_outs, temperature=1., with_var=True):\n",
    "        if len(self.models) == 1:\n",
    "            probs, attn, pvars, pentropy = self._decode_one(\n",
    "                tokens,\n",
    "                self.models[0],\n",
    "                encoder_outs[0] if self.has_encoder() else None,\n",
    "                self.incremental_states,\n",
    "                log_probs=True,\n",
    "                temperature=temperature,\n",
    "                with_var=True\n",
    "            )\n",
    "            if with_var:\n",
    "                probs_vars = torch.zeros(probs.size(), device=probs.device)\n",
    "                ens_var = probs.var(-1)\n",
    "                return probs, attn, probs_vars, probs_vars, torch.stack([pvars], dim=0), torch.stack([pentropy], dim=0), ens_var, ens_var\n",
    "            return probs, attn, None\n",
    "\n",
    "        log_probs = []\n",
    "        sing_var = []\n",
    "        sing_entropy = []\n",
    "        avg_attn = None\n",
    "        for model, encoder_out in zip(self.models, encoder_outs):\n",
    "            probs, attn, pvars, pentropy = self._decode_one(\n",
    "                tokens,\n",
    "                model,\n",
    "                encoder_out,\n",
    "                self.incremental_states,\n",
    "                log_probs=True,\n",
    "                temperature=temperature,\n",
    "                with_var=True\n",
    "            )\n",
    "            log_probs.append(probs)\n",
    "            print(probs.size())\n",
    "            sing_var.append(pvars)\n",
    "            sing_entropy.append(pentropy)\n",
    "            if attn is not None:\n",
    "                if avg_attn is None:\n",
    "                    avg_attn = attn\n",
    "                else:\n",
    "                    avg_attn.add_(attn)\n",
    "        \n",
    "        avg_probs = torch.logsumexp(torch.stack(log_probs, dim=0), dim=0) - math.log(len(self.models))\n",
    "\n",
    "        e_avg_probs = torch.exp(avg_probs)\n",
    "        ensemble_var = e_avg_probs.var(-1)\n",
    "        ensemble_entropy = shannon_entropy(e_avg_probs, -1)\n",
    "\n",
    "        e_probs = torch.exp(torch.stack(log_probs, dim=0))\n",
    "        print(e_probs.size())\n",
    "        probs_mean = e_probs.mean(dim=0)\n",
    "        probs_var = e_probs.var(dim=0)\n",
    "\n",
    "        sing_var = torch.stack(sing_var, dim=0)\n",
    "        sing_entropy = torch.stack(sing_entropy, dim=0)\n",
    "        if avg_attn is not None:\n",
    "            avg_attn.div_(len(self.models))\n",
    "        if with_var:\n",
    "            return avg_probs, avg_attn, probs_mean, probs_var, sing_var, sing_entropy, ensemble_var, ensemble_entropy\n",
    "        return avg_probs, avg_attn, probs_var\n",
    "\n",
    "    def _decode_one(\n",
    "        self, tokens, model, encoder_out, incremental_states, log_probs,\n",
    "        temperature=1.,\n",
    "        with_var=False\n",
    "    ):\n",
    "        if self.incremental_states is not None:\n",
    "            decoder_out = list(model.forward_decoder(\n",
    "                tokens, encoder_out=encoder_out, incremental_state=self.incremental_states[model],\n",
    "            ))\n",
    "        else:\n",
    "            decoder_out = list(model.forward_decoder(tokens, encoder_out=encoder_out))\n",
    "        decoder_out[0] = decoder_out[0][:, -1:, :]\n",
    "        if temperature != 1.:\n",
    "            decoder_out[0].div_(temperature)\n",
    "        attn = decoder_out[1] if len(decoder_out) > 1 else None\n",
    "        if type(attn) is dict:\n",
    "            attn = attn.get('attn', None)\n",
    "        if type(attn) is list:\n",
    "            attn = attn[0]\n",
    "        if attn is not None:\n",
    "            attn = attn[:, -1, :]\n",
    "        probs = model.get_normalized_probs(decoder_out, log_probs=log_probs)\n",
    "        probs = probs[:, -1, :]\n",
    "        e_probs = torch.exp(probs)\n",
    "        if with_var:\n",
    "            return probs, attn, e_probs.var(dim=1), shannon_entropy(e_probs, 1) \n",
    "        return probs, attn\n",
    "\n",
    "    def reorder_encoder_out(self, encoder_outs, new_order):\n",
    "        if not self.has_encoder():\n",
    "            return\n",
    "        return [\n",
    "            model.encoder.reorder_encoder_out(encoder_out, new_order)\n",
    "            for model, encoder_out in zip(self.models, encoder_outs)\n",
    "        ]\n",
    "\n",
    "    def reorder_incremental_state(self, new_order):\n",
    "        if self.incremental_states is None:\n",
    "            return\n",
    "        for model in self.models:\n",
    "            model.decoder.reorder_incremental_state(self.incremental_states[model], new_order)\n",
    "\n",
    "\n",
    "class SourceSequenceGenerator(SequenceGenerator):\n",
    "    @torch.no_grad()\n",
    "    def generate(self, models, sample, **kwargs):\n",
    "        \"\"\"Generate a batch of translations.\n",
    "\n",
    "        Args:\n",
    "            models (List[~fairseq.models.FairseqModel]): ensemble of models\n",
    "            sample (dict): batch\n",
    "            prefix_tokens (torch.LongTensor, optional): force decoder to begin\n",
    "                with these tokens\n",
    "            bos_token (int, optional): beginning of sentence token\n",
    "                (default: self.eos)\n",
    "        \"\"\"\n",
    "        model = EnsembleModel(models)\n",
    "        return self._generate(model, sample, **kwargs)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _generate(\n",
    "        self,\n",
    "        model,\n",
    "        sample,\n",
    "        prefix_tokens=None,\n",
    "        bos_token=None,\n",
    "        with_var=False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        if not self.retain_dropout:\n",
    "            model.eval()\n",
    "\n",
    "        # model.forward normally channels prev_output_tokens into the decoder\n",
    "        # separately, but SequenceGenerator directly calls model.encoder\n",
    "        encoder_input = {\n",
    "            k: v for k, v in sample['net_input'].items()\n",
    "            if k != 'prev_output_tokens'\n",
    "        }\n",
    "        \n",
    "        return_all_tokens = False\n",
    "        if 'return_all_tokens' in kwargs:\n",
    "            return_all_tokens = kwargs['return_all_tokens']\n",
    "            \n",
    "        src_tokens = encoder_input['src_tokens']\n",
    "        src_lengths = (src_tokens.ne(self.eos) & src_tokens.ne(self.pad)).long().sum(dim=1)\n",
    "        input_size = src_tokens.size()\n",
    "        # batch dimension goes first followed by source lengths\n",
    "        models_num = len(model.models)\n",
    "        bsz = input_size[0]\n",
    "        src_len = input_size[1]\n",
    "        beam_size = self.beam_size\n",
    "\n",
    "        if self.match_source_len:\n",
    "            max_len = src_lengths.max().item()\n",
    "        else:\n",
    "            max_len = min(\n",
    "                int(self.max_len_a * src_len + self.max_len_b),\n",
    "                # exclude the EOS marker\n",
    "                model.max_decoder_positions() - 1,\n",
    "            )\n",
    "        assert self.min_len <= max_len, 'min_len cannot be larger than max_len, please adjust these!'\n",
    "\n",
    "        # compute the encoder output for each beam\n",
    "        encoder_outs = model.forward_encoder(encoder_input)\n",
    "        new_order = torch.arange(bsz).view(-1, 1).repeat(1, beam_size).view(-1)\n",
    "        new_order = new_order.to(src_tokens.device).long()\n",
    "        encoder_outs = model.reorder_encoder_out(encoder_outs, new_order)\n",
    "\n",
    "        # initialize buffers\n",
    "        scores = src_tokens.new(bsz * beam_size, max_len + 1).float().fill_(0)\n",
    "        scores_buf = scores.clone()\n",
    "        tokens = src_tokens.new(bsz * beam_size, max_len + 2).long().fill_(self.pad)\n",
    "        tokens_buf = tokens.clone()\n",
    "        tokens[:, 0] = self.eos if bos_token is None else bos_token\n",
    "        attn, attn_buf = None, None\n",
    "\n",
    "        # The blacklist indicates candidates that should be ignored.\n",
    "        # For example, suppose we're sampling and have already finalized 2/5\n",
    "        # samples. Then the blacklist would mark 2 positions as being ignored,\n",
    "        # so that we only finalize the remaining 3 samples.\n",
    "        blacklist = src_tokens.new_zeros(bsz, beam_size).eq(-1)  # forward and backward-compatible False mask\n",
    "\n",
    "        # list of completed sentences\n",
    "        finalized = [[] for i in range(bsz)]\n",
    "        finished = [False for i in range(bsz)]\n",
    "        num_remaining_sent = bsz\n",
    "        \n",
    "        if return_all_tokens:\n",
    "            all_tokens = tokens.data.new(max_len + 2, 2 * bsz * beam_size).fill_(self.pad)\n",
    "            all_scores = scores.data.new(max_len + 2, 2 * bsz * beam_size).fill_(0)\n",
    "            all_softmaxes = torch.zeros(max_len + 2, beam_size, self.vocab_size)\n",
    "            all_vars = scores.data.new(max_len + 2, 2 * bsz * beam_size).fill_(0)\n",
    "            all_vars_vocab = torch.zeros(max_len + 2, beam_size, self.vocab_size)\n",
    "            all_means = scores.data.new(max_len + 2, 2 * bsz * beam_size).fill_(0)\n",
    "            all_sing_vars = torch.zeros(max_len + 2, models_num, beam_size)\n",
    "            all_sing_entropy = torch.zeros(max_len + 2, models_num, beam_size)\n",
    "            all_ens_vars = torch.zeros(max_len + 2, beam_size)\n",
    "            all_ens_entropy = torch.zeros(max_len + 2, beam_size)\n",
    "            is_finalized = torch.zeros(max_len + 2, bsz * beam_size, dtype=torch.uint8)\n",
    "            all_bbsz_idx = torch.zeros(max_len + 2, 2 * bsz * beam_size, dtype=torch.uint8)\n",
    "            all_lprobs = []\n",
    "\n",
    "        # number of candidate hypos per step\n",
    "        cand_size = 2 * beam_size  # 2 x beam size in case half are EOS\n",
    "\n",
    "        # offset arrays for converting between different indexing schemes\n",
    "        bbsz_offsets = (torch.arange(0, bsz) * beam_size).unsqueeze(1).type_as(tokens)\n",
    "        cand_offsets = torch.arange(0, cand_size).type_as(tokens)\n",
    "\n",
    "        # helper function for allocating buffers on the fly\n",
    "        buffers = {}\n",
    "\n",
    "        def buffer(name, type_of=tokens):  # noqa\n",
    "            if name not in buffers:\n",
    "                buffers[name] = type_of.new()\n",
    "            return buffers[name]\n",
    "\n",
    "        def is_finished(sent, step, unfin_idx):\n",
    "            \"\"\"\n",
    "            Check whether we've finished generation for a given sentence, by\n",
    "            comparing the worst score among finalized hypotheses to the best\n",
    "            possible score among unfinalized hypotheses.\n",
    "            \"\"\"\n",
    "            assert len(finalized[sent]) <= beam_size\n",
    "            if len(finalized[sent]) == beam_size or step == max_len:\n",
    "                return True\n",
    "            return False\n",
    "\n",
    "        def finalize_hypos(step, bbsz_idx, eos_scores):\n",
    "            \"\"\"\n",
    "            Finalize the given hypotheses at this step, while keeping the total\n",
    "            number of finalized hypotheses per sentence <= beam_size.\n",
    "\n",
    "            Note: the input must be in the desired finalization order, so that\n",
    "            hypotheses that appear earlier in the input are preferred to those\n",
    "            that appear later.\n",
    "\n",
    "            Args:\n",
    "                step: current time step\n",
    "                bbsz_idx: A vector of indices in the range [0, bsz*beam_size),\n",
    "                    indicating which hypotheses to finalize\n",
    "                eos_scores: A vector of the same size as bbsz_idx containing\n",
    "                    scores for each hypothesis\n",
    "            \"\"\"\n",
    "            assert bbsz_idx.numel() == eos_scores.numel()\n",
    "\n",
    "            # clone relevant token and attention tensors\n",
    "            tokens_clone = tokens.index_select(0, bbsz_idx)\n",
    "            tokens_clone = tokens_clone[:, 1:step + 2]  # skip the first index, which is EOS\n",
    "            assert not tokens_clone.eq(self.eos).any()\n",
    "            tokens_clone[:, step] = self.eos\n",
    "            attn_clone = attn.index_select(0, bbsz_idx)[:, :, 1:step+2] if attn is not None else None\n",
    "\n",
    "            # compute scores per token position\n",
    "            pos_scores = scores.index_select(0, bbsz_idx)[:, :step+1]\n",
    "            pos_scores[:, step] = eos_scores\n",
    "            # convert from cumulative to per-position scores\n",
    "            pos_scores[:, 1:] = pos_scores[:, 1:] - pos_scores[:, :-1]\n",
    "\n",
    "            # normalize sentence-level scores\n",
    "            if self.normalize_scores:\n",
    "                eos_scores /= (step + 1) ** self.len_penalty\n",
    "\n",
    "            cum_unfin = []\n",
    "            prev = 0\n",
    "            for f in finished:\n",
    "                if f:\n",
    "                    prev += 1\n",
    "                else:\n",
    "                    cum_unfin.append(prev)\n",
    "\n",
    "            sents_seen = set()\n",
    "            for i, (idx, score) in enumerate(zip(bbsz_idx.tolist(), eos_scores.tolist())):\n",
    "                unfin_idx = idx // beam_size\n",
    "                sent = unfin_idx + cum_unfin[unfin_idx]\n",
    "\n",
    "                sents_seen.add((sent, unfin_idx))\n",
    "\n",
    "                if self.match_source_len and step > src_lengths[unfin_idx]:\n",
    "                    score = -math.inf\n",
    "\n",
    "                def get_hypo():\n",
    "\n",
    "                    if attn_clone is not None:\n",
    "                        # remove padding tokens from attn scores\n",
    "                        hypo_attn = attn_clone[i]\n",
    "                    else:\n",
    "                        hypo_attn = None\n",
    "\n",
    "                    return {\n",
    "                        'tokens': tokens_clone[i],\n",
    "                        'score': score,\n",
    "                        'attention': hypo_attn,  # src_len x tgt_len\n",
    "                        'alignment': None,\n",
    "                        'positional_scores': pos_scores[i],\n",
    "                    }\n",
    "\n",
    "                if len(finalized[sent]) < beam_size:\n",
    "                    finalized[sent].append(get_hypo())\n",
    "                    if return_all_tokens:\n",
    "                        is_finalized[step, idx] = 1\n",
    "\n",
    "            newly_finished = []\n",
    "            for sent, unfin_idx in sents_seen:\n",
    "                # check termination conditions for this sentence\n",
    "                if not finished[sent] and is_finished(sent, step, unfin_idx):\n",
    "                    finished[sent] = True\n",
    "                    newly_finished.append(unfin_idx)\n",
    "            return newly_finished\n",
    "\n",
    "        reorder_state = None\n",
    "        batch_idxs = None\n",
    "        for step in range(max_len + 1):  # one extra step for EOS marker\n",
    "            # reorder decoder internal states based on the prev choice of beams\n",
    "            if reorder_state is not None:\n",
    "                if batch_idxs is not None:\n",
    "                    # update beam indices to take into account removed sentences\n",
    "                    corr = batch_idxs - torch.arange(batch_idxs.numel()).type_as(batch_idxs)\n",
    "                    reorder_state.view(-1, beam_size).add_(corr.unsqueeze(-1) * beam_size)\n",
    "                model.reorder_incremental_state(reorder_state)\n",
    "                encoder_outs = model.reorder_encoder_out(encoder_outs, reorder_state)\n",
    "\n",
    "            lprobs, avg_attn_scores, probs_means, probs_vars, sing_vars, sing_entropy, ens_vars, ens_entropy = model.forward_decoder(\n",
    "                tokens[:, :step + 1], encoder_outs, temperature=self.temperature, with_var=True\n",
    "            )\n",
    "            lprobs[lprobs != lprobs] = -math.inf\n",
    "\n",
    "            lprobs[:, self.pad] = -math.inf  # never select pad\n",
    "            lprobs[:, self.unk] -= self.unk_penalty  # apply unk penalty\n",
    "\n",
    "            # handle max length constraint\n",
    "            if step >= max_len:\n",
    "                lprobs[:, :self.eos] = -math.inf\n",
    "                lprobs[:, self.eos + 1:] = -math.inf\n",
    "\n",
    "            # handle prefix tokens (possibly with different lengths)\n",
    "            if prefix_tokens is not None and step < prefix_tokens.size(1) and step < max_len:\n",
    "                prefix_toks = prefix_tokens[:, step].unsqueeze(-1).repeat(1, beam_size).view(-1)\n",
    "                prefix_lprobs = lprobs.gather(-1, prefix_toks.unsqueeze(-1))\n",
    "                prefix_vars = probs_vars.gather(-1, prefix_toks.unsqueeze(-1))\n",
    "                prefix_means = probs_means.gather(-1, prefix_toks.unsqueeze(-1))\n",
    "                prefix_mask = prefix_toks.ne(self.pad)\n",
    "                lprobs[prefix_mask] = -math.inf\n",
    "                # TODO\n",
    "                lprobs[prefix_mask] = lprobs[prefix_mask].scatter_(\n",
    "                    -1, prefix_toks[prefix_mask].unsqueeze(-1), prefix_lprobs[prefix_mask]\n",
    "                )\n",
    "                probs_vars[prefix_mask] = probs_vars[prefix_mask].scatter_(\n",
    "                    -1, prefix_toks[prefix_mask].unsqueeze(-1), prefix_vars[prefix_mask]\n",
    "                )\n",
    "                probs_means[prefix_mask] = probs_means[prefix_mask].scatter_(\n",
    "                    -1, prefix_toks[prefix_mask].unsqueeze(-1), prefix_means[prefix_mask]\n",
    "                )\n",
    "                # if prefix includes eos, then we should make sure tokens and\n",
    "                # scores are the same across all beams\n",
    "                eos_mask = prefix_toks.eq(self.eos)\n",
    "                if eos_mask.any():\n",
    "                    # validate that the first beam matches the prefix\n",
    "                    first_beam = tokens[eos_mask].view(-1, beam_size, tokens.size(-1))[:, 0, 1:step + 1]\n",
    "                    eos_mask_batch_dim = eos_mask.view(-1, beam_size)[:, 0]\n",
    "                    target_prefix = prefix_tokens[eos_mask_batch_dim][:, :step]\n",
    "                    assert (first_beam == target_prefix).all()\n",
    "\n",
    "                    def replicate_first_beam(tensor, mask):\n",
    "                        tensor = tensor.view(-1, beam_size, tensor.size(-1))\n",
    "                        tensor[mask] = tensor[mask][:, :1, :]\n",
    "                        return tensor.view(-1, tensor.size(-1))\n",
    "\n",
    "                    # copy tokens, scores and lprobs from the first beam to all beams\n",
    "                    tokens = replicate_first_beam(tokens, eos_mask_batch_dim)\n",
    "                    scores = replicate_first_beam(scores, eos_mask_batch_dim)\n",
    "                    lprobs = replicate_first_beam(lprobs, eos_mask_batch_dim)\n",
    "                    probs_vars = replicate_first_beam(probs_vars, eos_mask_batch_dim)\n",
    "                    probs_means = replicate_first_beam(probs_means, eos_mask_batch_dim)\n",
    "            elif step < self.min_len:\n",
    "                # minimum length constraint (does not apply if using prefix_tokens)\n",
    "                lprobs[:, self.eos] = -math.inf\n",
    "\n",
    "            if self.no_repeat_ngram_size > 0:\n",
    "                # for each beam and batch sentence, generate a list of previous ngrams\n",
    "                gen_ngrams = [{} for bbsz_idx in range(bsz * beam_size)]\n",
    "                for bbsz_idx in range(bsz * beam_size):\n",
    "                    gen_tokens = tokens[bbsz_idx].tolist()\n",
    "                    for ngram in zip(*[gen_tokens[i:] for i in range(self.no_repeat_ngram_size)]):\n",
    "                        gen_ngrams[bbsz_idx][tuple(ngram[:-1])] =                                 gen_ngrams[bbsz_idx].get(tuple(ngram[:-1]), []) + [ngram[-1]]\n",
    "\n",
    "            # Record attention scores\n",
    "            if type(avg_attn_scores) is list:\n",
    "                avg_attn_scores = avg_attn_scores[0]\n",
    "            if avg_attn_scores is not None:\n",
    "                if attn is None:\n",
    "                    attn = scores.new(bsz * beam_size, src_tokens.size(1), max_len + 2)\n",
    "                    attn_buf = attn.clone()\n",
    "                attn[:, :, step + 1].copy_(avg_attn_scores)\n",
    "\n",
    "            scores = scores.type_as(lprobs)\n",
    "            scores_buf = scores_buf.type_as(lprobs)\n",
    "            eos_bbsz_idx = buffer('eos_bbsz_idx')\n",
    "            eos_scores = buffer('eos_scores', type_of=scores)\n",
    "\n",
    "            self.search.set_src_lengths(src_lengths)\n",
    "\n",
    "            if self.no_repeat_ngram_size > 0:\n",
    "                def calculate_banned_tokens(bbsz_idx):\n",
    "                    # before decoding the next token, prevent decoding of ngrams that have already appeared\n",
    "                    ngram_index = tuple(tokens[bbsz_idx, step + 2 - self.no_repeat_ngram_size:step + 1].tolist())\n",
    "                    return gen_ngrams[bbsz_idx].get(ngram_index, [])\n",
    "\n",
    "                if step + 2 - self.no_repeat_ngram_size >= 0:\n",
    "                    # no banned tokens if we haven't generated no_repeat_ngram_size tokens yet\n",
    "                    banned_tokens = [calculate_banned_tokens(bbsz_idx) for bbsz_idx in range(bsz * beam_size)]\n",
    "                else:\n",
    "                    banned_tokens = [[] for bbsz_idx in range(bsz * beam_size)]\n",
    "\n",
    "                for bbsz_idx in range(bsz * beam_size):\n",
    "                    lprobs[bbsz_idx, banned_tokens[bbsz_idx]] = -math.inf\n",
    "\n",
    "\n",
    "            clean_lprobs = lprobs.clone().detach()\n",
    "            cand_scores, cand_indices, cand_beams = self.search.step(\n",
    "                step,\n",
    "                lprobs.view(bsz, -1, self.vocab_size),\n",
    "                scores.view(bsz, beam_size, -1)[:, :, :step],\n",
    "            )\n",
    "            \n",
    "            ishape = cand_indices.shape[1]\n",
    "            cand_vars = torch.ones((bsz, ishape))\n",
    "            if with_var:\n",
    "                boffsets = (torch.cumsum(\n",
    "                    torch.full((bsz, ), ishape, dtype=torch.int64, device=cand_indices.device) - ishape,\n",
    "                    dim=0\n",
    "                )).unsqueeze_(-1).T\n",
    "\n",
    "                boffset_idxs = (cand_indices + boffsets).flatten()\n",
    "                cand_vars = probs_vars.flatten()[boffset_idxs].view(bsz, -1)\n",
    "                cand_means = probs_means.flatten()[boffset_idxs].view(bsz, -1)\n",
    "\n",
    "            # cand_bbsz_idx contains beam indices for the top candidate\n",
    "            # hypotheses, with a range of values: [0, bsz*beam_size),\n",
    "            # and dimensions: [bsz, cand_size]\n",
    "            cand_bbsz_idx = cand_beams.add(bbsz_offsets)\n",
    "            \n",
    "            if return_all_tokens:\n",
    "                all_scores[step + 1] = cand_scores #-scores[cand_beams,step-1]\n",
    "                all_softmaxes[step + 1] = clean_lprobs\n",
    "                all_vars[step + 1] = cand_vars\n",
    "                all_vars_vocab[step + 1] = probs_vars\n",
    "                all_means[step + 1] = cand_means\n",
    "                all_tokens[step + 1] = cand_indices\n",
    "                all_bbsz_idx[step] = cand_bbsz_idx\n",
    "                all_sing_vars[step + 1] = sing_vars\n",
    "                all_sing_entropy[step + 1] = sing_entropy\n",
    "                all_ens_vars[step + 1] = ens_vars\n",
    "                all_ens_entropy[step + 1] = ens_entropy\n",
    "\n",
    "\n",
    "            # finalize hypotheses that end in eos, except for blacklisted ones\n",
    "            # or candidates with a score of -inf\n",
    "            eos_mask = cand_indices.eq(self.eos) & cand_scores.ne(-math.inf)\n",
    "            eos_mask[:, :beam_size][blacklist] = 0\n",
    "\n",
    "            # only consider eos when it's among the top beam_size indices\n",
    "            torch.masked_select(\n",
    "                cand_bbsz_idx[:, :beam_size],\n",
    "                mask=eos_mask[:, :beam_size],\n",
    "                out=eos_bbsz_idx,\n",
    "            )\n",
    "\n",
    "            finalized_sents = set()\n",
    "            if eos_bbsz_idx.numel() > 0:\n",
    "                torch.masked_select(\n",
    "                    cand_scores[:, :beam_size],\n",
    "                    mask=eos_mask[:, :beam_size],\n",
    "                    out=eos_scores,\n",
    "                )\n",
    "                finalized_sents = finalize_hypos(step, eos_bbsz_idx, eos_scores)\n",
    "                num_remaining_sent -= len(finalized_sents)\n",
    "\n",
    "            assert num_remaining_sent >= 0\n",
    "            if num_remaining_sent == 0:\n",
    "                break\n",
    "            assert step < max_len\n",
    "\n",
    "            if len(finalized_sents) > 0:\n",
    "                new_bsz = bsz - len(finalized_sents)\n",
    "\n",
    "                # construct batch_idxs which holds indices of batches to keep for the next pass\n",
    "                batch_mask = cand_indices.new_ones(bsz)\n",
    "                batch_mask[cand_indices.new(finalized_sents)] = 0\n",
    "                batch_idxs = batch_mask.nonzero().squeeze(-1)\n",
    "\n",
    "                eos_mask = eos_mask[batch_idxs]\n",
    "                cand_beams = cand_beams[batch_idxs]\n",
    "                bbsz_offsets.resize_(new_bsz, 1)\n",
    "                cand_bbsz_idx = cand_beams.add(bbsz_offsets)\n",
    "                cand_scores = cand_scores[batch_idxs]\n",
    "                cand_indices = cand_indices[batch_idxs]\n",
    "                if prefix_tokens is not None:\n",
    "                    prefix_tokens = prefix_tokens[batch_idxs]\n",
    "                src_lengths = src_lengths[batch_idxs]\n",
    "                blacklist = blacklist[batch_idxs]\n",
    "\n",
    "                scores = scores.view(bsz, -1)[batch_idxs].view(new_bsz * beam_size, -1)\n",
    "                scores_buf.resize_as_(scores)\n",
    "                tokens = tokens.view(bsz, -1)[batch_idxs].view(new_bsz * beam_size, -1)\n",
    "                tokens_buf.resize_as_(tokens)\n",
    "                if attn is not None:\n",
    "                    attn = attn.view(bsz, -1)[batch_idxs].view(new_bsz * beam_size, attn.size(1), -1)\n",
    "                    attn_buf.resize_as_(attn)\n",
    "                bsz = new_bsz\n",
    "            else:\n",
    "                batch_idxs = None\n",
    "\n",
    "            # Set active_mask so that values > cand_size indicate eos or\n",
    "            # blacklisted hypos and values < cand_size indicate candidate\n",
    "            # active hypos. After this, the min values per row are the top\n",
    "            # candidate active hypos.\n",
    "            active_mask = buffer('active_mask')\n",
    "            eos_mask[:, :beam_size] |= blacklist\n",
    "            torch.add(\n",
    "                eos_mask.type_as(cand_offsets) * cand_size,\n",
    "                cand_offsets[:eos_mask.size(1)],\n",
    "                out=active_mask,\n",
    "            )\n",
    "\n",
    "            # get the top beam_size active hypotheses, which are just the hypos\n",
    "            # with the smallest values in active_mask\n",
    "            active_hypos, new_blacklist = buffer('active_hypos'), buffer('new_blacklist')\n",
    "            torch.topk(\n",
    "                active_mask, k=beam_size, dim=1, largest=False,\n",
    "                out=(new_blacklist, active_hypos)\n",
    "            )\n",
    "\n",
    "            # update blacklist to ignore any finalized hypos\n",
    "            blacklist = new_blacklist.ge(cand_size)[:, :beam_size]\n",
    "            assert (~blacklist).any(dim=1).all()\n",
    "\n",
    "            active_bbsz_idx = buffer('active_bbsz_idx')\n",
    "            torch.gather(\n",
    "                cand_bbsz_idx, dim=1, index=active_hypos,\n",
    "                out=active_bbsz_idx,\n",
    "            )\n",
    "            active_scores = torch.gather(\n",
    "                cand_scores, dim=1, index=active_hypos,\n",
    "                out=scores[:, step].view(bsz, beam_size),\n",
    "            )\n",
    "\n",
    "            active_bbsz_idx = active_bbsz_idx.view(-1)\n",
    "            active_scores = active_scores.view(-1)\n",
    "\n",
    "            # copy tokens and scores for active hypotheses\n",
    "            torch.index_select(\n",
    "                tokens[:, :step + 1], dim=0, index=active_bbsz_idx,\n",
    "                out=tokens_buf[:, :step + 1],\n",
    "            )\n",
    "            torch.gather(\n",
    "                cand_indices, dim=1, index=active_hypos,\n",
    "                out=tokens_buf.view(bsz, beam_size, -1)[:, :, step + 1],\n",
    "            )\n",
    "            if step > 0:\n",
    "                torch.index_select(\n",
    "                    scores[:, :step], dim=0, index=active_bbsz_idx,\n",
    "                    out=scores_buf[:, :step],\n",
    "                )\n",
    "            torch.gather(\n",
    "                cand_scores, dim=1, index=active_hypos,\n",
    "                out=scores_buf.view(bsz, beam_size, -1)[:, :, step],\n",
    "            )\n",
    "\n",
    "            # copy attention for active hypotheses\n",
    "            if attn is not None:\n",
    "                torch.index_select(\n",
    "                    attn[:, :, :step + 2], dim=0, index=active_bbsz_idx,\n",
    "                    out=attn_buf[:, :, :step + 2],\n",
    "                )\n",
    "\n",
    "            # swap buffers\n",
    "            tokens, tokens_buf = tokens_buf, tokens\n",
    "            scores, scores_buf = scores_buf, scores\n",
    "            if attn is not None:\n",
    "                attn, attn_buf = attn_buf, attn\n",
    "\n",
    "            # reorder incremental state in decoder\n",
    "            reorder_state = active_bbsz_idx\n",
    "\n",
    "        # sort by score descending\n",
    "        for sent in range(len(finalized)):\n",
    "            finalized[sent] = sorted(finalized[sent], key=lambda r: r['score'], reverse=True)\n",
    "            \n",
    "        if return_all_tokens:\n",
    "            all_scores = all_scores[:step + 2]\n",
    "            all_softmaxes = all_softmaxes[:step + 2]\n",
    "            all_vars = all_vars[:step + 2]\n",
    "            all_vars_vocab = all_vars_vocab[:step + 2]\n",
    "            all_means = all_means[:step + 2]\n",
    "            all_sing_vars = all_sing_vars[:step + 2]\n",
    "            all_sing_entropy = all_sing_entropy[:step + 2]\n",
    "            all_ens_vars = all_ens_vars[:step + 2]\n",
    "            all_ens_entropy = all_ens_entropy[:step + 2]\n",
    "            if with_var:\n",
    "                    return finalized, all_tokens[:step + 2].cpu(), all_scores.cpu(), is_finalized[:step + 1].cpu(), all_bbsz_idx[:step+1].cpu(), all_softmaxes.cpu(), all_means.cpu(), all_vars.cpu(), all_vars_vocab.cpu(), all_sing_vars.cpu(), all_sing_entropy.cpu(), all_ens_vars.cpu(), all_ens_entropy.cpu()\n",
    "                \n",
    "            return finalized, all_tokens[:step + 2].cpu(), all_scores.cpu(), is_finalized[:step + 1].cpu(), all_bbsz_idx[:step+1].cpu()\n",
    "        return finalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'tmp_stats' from '/home/dkuznetsov/notebook/course_paper/experiments/beam_width/tmp_stats.py'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(tmp_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = tmp_stats.SourceSequenceGenerator(\n",
    "    tgt_dict=tgt_dict,\n",
    "    beam_size=args.beam,\n",
    "    min_len=args.min_len,\n",
    "    normalize_scores=(not args.unnormalized),\n",
    "    len_penalty=args.lenpen,\n",
    "    unk_penalty=args.unkpen\n",
    "    # ,\n",
    "    # ,\n",
    "    # sampling_temperature=args.sampling_temperature,\n",
    "    # diverse_beam_groups=args.diverse_beam_groups,\n",
    "    # diverse_beam_strength=args.diverse_beam_strength\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n"
     ]
    }
   ],
   "source": [
    "sent = 'It should be noted that the marine environment is the least known of environments .'\n",
    "sent = 'Greetings, my name is Dmitriy'\n",
    "ref_sent = 'Привет, меня зовут Дмитрий'\n",
    "# sent = 'Sort of like what stylist Lino Villaventura organized .'\n",
    "\n",
    "all_vars = None\n",
    "all_sing_vars = None\n",
    "all_ens_vars = None\n",
    "all_softmaxes = None\n",
    "for batch in make_batches(prepare_input(sent, l='en'), args, task, max_positions):\n",
    "    encoder_input = {'net_input': {'src_tokens': batch.src_tokens, 'src_lengths': batch.src_lengths}}\n",
    "\n",
    "    translations, all_tokens, all_scores, is_finalized, all_bbsz_idx, all_stats = translator.generate(\n",
    "        models=models,\n",
    "        sample=encoder_input,\n",
    "        return_all_tokens=True,\n",
    "        with_stats=True\n",
    "    )\n",
    "    \n",
    "# if all_vars is None:\n",
    "#     all_vars = all_scores\n",
    "# if all_sing_vars is None:\n",
    "#     all_sing_vars = torch.zeros((all_vars.shape[0], len(models), int(BEAM)))\n",
    "# if all_ens_vars is None:\n",
    "#     all_ens_vars = torch.zeros((all_vars.shape[0], int(BEAM)))\n",
    "#     \n",
    "# all_sing_vars = all_sing_vars.mean(-1)\n",
    "# all_sing_entropy = all_sing_entropy.mean(-1)\n",
    "# all_ens_vars = all_ens_vars.mean(-1)\n",
    "# all_ens_entropy = all_ens_entropy.mean(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_TOKENS_LIST = [1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(TOP_TOKENS_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.LongTensor([\n",
    "    [\n",
    "        [1, 2, 3],\n",
    "        [4, 5, 6]\n",
    "    ],\n",
    "    [\n",
    "        [7, 8, 9],\n",
    "        [10, 11, 12]\n",
    "    ]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "values, indices = torch.topk(a, 2, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3,  6],\n",
       "        [ 9, 12]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values[..., 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([-1.9607, -2.7230, -2.8193, -3.7478, -4.1105]),\n",
       "indices=tensor([0, 1, 2, 3, 4]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.topk(all_scores[5], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "764"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_tokens[a[0]].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.arange(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(764)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(764)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens.extend([tgt_tokens[torch.LongTensor([0])].tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[764]]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = all_stats['inens_probs_var'][..., 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.softmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   nan, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.log_softmax(all_scores - torch.log(v), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = all_stats['inens_probs_var']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_tokens = translations[0][0]['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.BoolTensor([True, True, True] + [False] * 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v[:max_len][mask, ..., tgt_tokens[:max_len][mask]].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stats.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stats['inens_probs_mean'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = all_stats['inens_probs_mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = all_stats['inens_probs_var']\n",
    "d = all_stats['inens_probs_dist']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stats['inens_probs_dist'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 666"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v[7][4][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(d[7][4][:, idx]).var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[0, ..., 555].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = all_stats['softmax_probs_var_ens']\n",
    "d = all_stats['softmax_probs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 3\n",
    "idx = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v[step][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(d[step][idx]).var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_bbsz_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(mean[torch.arange(10), :, all_tokens]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation = translations[0][0]['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator.temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_means.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(all_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator.no_repeat_ngram_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_dict.string(translations[0][0]['tokens'], bpe_symbol='@@ ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_dict.eos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_dict.string([2], bpe_symbol='@@ ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator.no_repeat_ngram_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inens_dist.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs, inens_mean, inens_mean_vocab, inens_var, inens_var_vocab, ens_softmaxes, inens_dist = tmp_e.get_translation_stats(tgt_tokens, all_tokens, all_bbsz_idx, all_scores, all_means, all_means_vocab     , all_vars, all_vars_vocab, all_softmaxes, inens_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inens_dist.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inens_dist[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_means.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ens_vars.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ens_entropy.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sing_vars.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sing_entropy.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vars.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_means.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sing_vars.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sing_entropy.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vars.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_means.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_tokens = tgt_dict.encode_line(prepare_input(ref_sent, 'en')[0], add_if_not_exist=False).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tgt_dict.string(translations[0][0]['tokens'], bpe_symbol='@@ '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def get_stats_distribution(ref_tokens, tgt_tokens, token_cmp, *args):\n",
    "    ref_len = ref_tokens.shape[0]\n",
    "    tgt_len = tgt_tokens.shape[0]\n",
    "    max_len = min(ref_len, tgt_len)\n",
    "    \n",
    "    mask = token_cmp(ref_tokens[:max_len], tgt_tokens[:max_len])\n",
    "    stats = dict()\n",
    "    for name, score in args:\n",
    "        stats[name] = np.array(score[:max_len][mask].tolist())\n",
    "        \n",
    "    return stats\n",
    "        \n",
    "\n",
    "def get_translation_stats(tgt_tokens, beam_tokens, beam_scores, beam_vars):\n",
    "    tgt_len = tgt_tokens.shape[0]\n",
    "    tgt_tokens = tgt_tokens.view(tgt_len, -1)\n",
    "    \n",
    "    mask = beam_tokens[1:tgt_len + 1] == tgt_tokens\n",
    "    \n",
    "    tscores = beam_scores[1:tgt_len + 1][mask]\n",
    "    tvars = beam_vars[1:tgt_len + 1][mask]\n",
    "    \n",
    "    return tscores, tvars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tscores, tvars = get_translation_stats(translations[0][0]['tokens'], all_tokens, all_scores, all_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_stats_distribution(\n",
    "    ref_tokens,\n",
    "    translations[0][0]['tokens'],\n",
    "    lambda x, y: x == y,\n",
    "    ('prob', tscores),\n",
    "    ('inensemble_var', tvars),\n",
    "    ('m1_svar', all_sing_vars[:, 0]),\n",
    "    ('m2_svar', all_sing_vars[:, 1]),\n",
    "    ('m3_svar', all_sing_vars[:, 2]),\n",
    "    ('m4_svar', all_sing_vars[:, 3]),\n",
    "    ('ens_svar', all_ens_vars)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_dict.string([764])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tscores.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvars.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sing_vars[:, 0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ens_vars.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "sns.distplot(np.exp(stats['positive']['vars']), label='positive', color='red')\n",
    "sns.distplot(np.exp(stats['negative']['vars']), label='negative', color='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tgt_dict.encode_line(prepare_input(ref_sent, 'en')[0], add_if_not_exist=False).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translations[0][0]['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_dict.string(t, bpe_symbol='@@ ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_tokens = translations[0][0]['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_len = tgt_tokens.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = all_tokens[1:tgt_len + 1] == tgt_tokens.view(tgt_len, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vars[1:tgt_len + 1][mask].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens[1:tgt_len + 1][mask].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_len = ref_tokens.size(0)\n",
    "t_len = tgt_tokens.size(0)\n",
    "ln = min(r_len, t_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = ref_tokens[:ln] == tgt_tokens[:ln]\n",
    "idxs = torch.arange(ln)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask[-2] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fidx = idxs[~mask][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fidx.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask[fidx:] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sidx = idxs[~mask][0]\n",
    "mask[sidx:] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.cat((\n",
    "    mask,\n",
    "    torch.full((t_len - ln,), False, dtype=torch.bool)\n",
    ")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix = tgt_tokens[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_beam_search(beam_size=None):\n",
    "    beam_size=args.beam\n",
    "    if beam_size is not None:\n",
    "        beam_size = beam_size\n",
    "    x = []\n",
    "    y = []\n",
    "    name = []\n",
    "    score = []\n",
    "    var = []\n",
    "    inds_by_step = []\n",
    "    inds_by_step_noeos = []\n",
    "    parent_index = []\n",
    "    linew = []\n",
    "\n",
    "    edge_to_ind = {}\n",
    "    lwidth = []\n",
    "\n",
    "    cur_ind = 0\n",
    "\n",
    "    for step, tokens in enumerate(all_tokens):\n",
    "        if step == 0:\n",
    "            inds_by_step.append([cur_ind])\n",
    "            inds_by_step_noeos.append([0])\n",
    "            cur_ind += 1\n",
    "            x.append(step)\n",
    "            y.append((beam_size + 1) / 2)\n",
    "            name.append(tgt_dict.pad_word)\n",
    "            score.append(0)\n",
    "            var.append(0)\n",
    "            linew.append(1)\n",
    "        else:\n",
    "            non_eos_encountered=0\n",
    "            cur_inds = []\n",
    "            cur_noeos = []\n",
    "            for i, beam_tk in enumerate(tokens):\n",
    "                cur_inds.append(cur_ind)\n",
    "                cur_ind += 1\n",
    "                x.append(step)\n",
    "                y.append(i + 1)\n",
    "                name.append(tgt_dict.symbols[beam_tk].replace('@@',''))\n",
    "                score.append(all_scores[step, i].item() / (step ** args.lenpen))\n",
    "                var.append(all_vars[step, i].item())\n",
    "                prev_beam_ind = all_bbsz_idx[step - 1, i].item()\n",
    "                parent_index.append(\n",
    "                    inds_by_step[step - 1][inds_by_step_noeos[step - 1][prev_beam_ind]]\n",
    "                )\n",
    "                edge = (\n",
    "                    all_tokens[step - 1, inds_by_step_noeos[step - 1][prev_beam_ind]].item(), \n",
    "                    beam_tk.item()\n",
    "                )\n",
    "                for j, translation in enumerate(translations[0]):\n",
    "                    if step == 1:\n",
    "                        res_edge = (tgt_dict.pad_index, translation['tokens'][step - 1].item())\n",
    "                    else:\n",
    "                        res_edge = tuple(translation['tokens'][step - 2: step].cpu().numpy())\n",
    "                    if edge == res_edge:\n",
    "                        lwidth.append(1 + 0.5 * (beam_size - j))\n",
    "                        break\n",
    "                else:\n",
    "                    lwidth.append(1)\n",
    "                edge_to_ind[edge] = len(lwidth)\n",
    "                if i < beam_size and is_finalized[step - 1, i].item():\n",
    "                    linew.append(2)\n",
    "                else:\n",
    "                    linew.append(1)\n",
    "                if beam_tk != tgt_dict.eos_index:\n",
    "                    non_eos_encountered += 1\n",
    "                    cur_noeos.append(i)\n",
    "                    if non_eos_encountered == beam_size:\n",
    "                        break\n",
    "            inds_by_step.append(cur_inds)\n",
    "            inds_by_step_noeos.append(cur_noeos)\n",
    "\n",
    "\n",
    "    index = sum(inds_by_step, [])            \n",
    "\n",
    "    src=ColumnDataSource(data=dict(x=x, y=y, name=name, index=index, score=np.exp(score), var=var, linew=linew))\n",
    "\n",
    "    plot = figure(\n",
    "        x_range=(min(x) - 0.25, max(x) + 1),\n",
    "        y_range=(min(y) - 1, max(y) + 1),\n",
    "        tools=[\n",
    "            HoverTool(tooltips=[\n",
    "                ('Name','@name'),\n",
    "                ('Score','@score'),\n",
    "                ('Var', '@var')\n",
    "            ]),\n",
    "            PanTool(),\n",
    "            BoxZoomTool()\n",
    "        ],\n",
    "        toolbar_location=None,\n",
    "        plot_width=900,\n",
    "        plot_height=600,\n",
    "        x_axis_type=None,\n",
    "        y_axis_type=None\n",
    "    )\n",
    "    graph = GraphRenderer()\n",
    "\n",
    "    graph.node_renderer.data_source.data=src.data\n",
    "    graph.node_renderer.glyph = Circle(radius=0.1, fill_color='#9999ee', line_width='linew')\n",
    "\n",
    "    graph.edge_renderer.data_source.data = dict(\n",
    "        start=parent_index,\n",
    "        end=index[1:],\n",
    "        width=lwidth\n",
    "    )\n",
    "    graph.edge_renderer.glyph = MultiLine()\n",
    "\n",
    "    graph_layout = dict(zip(index, zip(x, y)))\n",
    "    graph.layout_provider = StaticLayoutProvider(graph_layout=graph_layout)\n",
    "\n",
    "    labels = LabelSet(\n",
    "        x='x',\n",
    "        y='y',\n",
    "        text='name',\n",
    "        level='glyph',\n",
    "        x_offset=-15,\n",
    "        y_offset=20,\n",
    "        source=src\n",
    "    )\n",
    "\n",
    "    plot.renderers.append(graph)\n",
    "    plot.add_layout(labels)\n",
    "\n",
    "    ticker = SingleIntervalTicker(interval=1, num_minor_ticks=5)\n",
    "    xaxis = LinearAxis(ticker=ticker)\n",
    "    plot.add_layout(xaxis, 'below')\n",
    "\n",
    "    output_file('graph.html')\n",
    "    show(plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print_beam_search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step, mvars in enumerate(all_sing_vars):\n",
    "    print(step, 'step')\n",
    "    for midx, var in enumerate(mvars):\n",
    "        print('\\t', 'model', midx + 1, 'softmax var - ', var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step, var in enumerate(all_ens_vars):\n",
    "    print(step, 'step')\n",
    "    print('\\t', 'ensemble sofrmax var - ', var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task.load_dataset(args.gen_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itr = task.get_batch_iterator(\n",
    "        dataset=task.dataset(args.gen_subset),\n",
    "        max_tokens=args.max_tokens,\n",
    "        max_sentences=args.max_sentences,\n",
    "        max_positions=max_positions,\n",
    "        ignore_invalid_inputs=args.skip_invalid_size_inputs_valid_test,\n",
    "        required_batch_size_multiple=args.required_batch_size_multiple,\n",
    "        num_shards=args.num_shards,\n",
    "        shard_id=args.shard_id,\n",
    "        num_workers=args.num_workers,\n",
    ").next_epoch_itr(shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in itr:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress = progress_bar.build_progress_bar(\n",
    "        args,\n",
    "        itr\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm.tqdm(range(100)):\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in progress:\n",
    "    print('*')\n",
    "    d = sample\n",
    "    print('*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s['target'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fairseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fairseq.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in itr:\n",
    "    b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {'a': 1, 'b': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test', 'w') as stream_output:\n",
    "    json.dump(a, stream_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test') as stream_output:\n",
    "    t= json.load(stream_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.extend([2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.zeros((1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.zeros((5, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.view(1, a.shape[0], -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
